{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweets Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPLT3tEdHhiaXD/ku8al66t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidGlezGmz/Natural-Language-Processing-NLP-/blob/main/Tweets_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZUwYFivtA0s"
      },
      "source": [
        "Unsupervised NLP using GUSE and K-means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BV0_fsrss2Lz"
      },
      "source": [
        "$ r_{i,k} =\\frac{e^{-\\beta || x_i - m_k||^2}}{\\sum_{i=0}^{N} e^{-\\beta || x_i - m_k||^2}} $ \n",
        "\n",
        "$m_k=\\frac{\\sum_i^N r_{i,k} x_i}{\\sum r_{s,k}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85vdQLwKs4zf"
      },
      "outputs": [],
      "source": [
        "def Responsability(X,means,beta=1e-3,*args,**kwargs):\n",
        "  diff=[]\n",
        "  responsibilities=[]\n",
        "\n",
        "  for mean in means:\n",
        "    diff =X -mean\n",
        "    dist =(np.sum(diff ** 2,axis=1,keepdims=True))\n",
        "\n",
        "    numerator= np.exp(-(beta*dist))\n",
        "    denominator=np.sum(numerator)\n",
        "    responsibility=-numerator/denominator\n",
        "\n",
        "    responsibilities.append(responsibility)\n",
        "  return np.hstack(responsibilities)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "XJBxwj5ptFdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numpy.linalg as lg\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "2xKqumrKtHW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "embedder = hub.load(module_url)"
      ],
      "metadata": {
        "id": "G1Ytb3bLtI4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TextPreProcessing(sentence):\n",
        "  text = sentence.lower().strip()\n",
        "  text_p = \"\".join(char for char in text if char not in string.punctuation)\n",
        "  clean_sentence = text_dn = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',text_p)\n",
        "  return clean_sentence"
      ],
      "metadata": {
        "id": "QNvaBmZrtJaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed(sentence):\n",
        "  sentence_in = [sentence]\n",
        "  return embedder(sentence_in)"
      ],
      "metadata": {
        "id": "C-aZPeYwtK6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_load(filename):\n",
        "  data = pd.read_csv(filename, header=0, encoding='iso-8859-1')\n",
        "  X = data.Tweet\n",
        "  return X"
      ],
      "metadata": {
        "id": "O4LB3LWutMNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LoadVectors(X):\n",
        "  Dict_of_Sentences = dict()\n",
        "  for i in range(len(X)):\n",
        "    Dict_of_Sentences[i] = {'sentence': X[i], 'vector': embed(TextPreProcessing(X[i]))}\n",
        "  return Dict_of_Sentences"
      ],
      "metadata": {
        "id": "KJtR6LAjtNnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_load('https://raw.githubusercontent.com/DavidGlezGmz/K-Means/main/data_elonmusk2.csv')"
      ],
      "metadata": {
        "id": "Yn4yATaPtPL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "yjsSSTaztScr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "BId59hNztT7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "My_NLP_dict = LoadVectors(X)"
      ],
      "metadata": {
        "id": "JBmHQSLhtVjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "My_NLP_dict"
      ],
      "metadata": {
        "id": "fEdhQ116tWx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class K_Means:\n",
        "  def __init__(self, k=3, distance_func=Distance, beta=None):\n",
        "    self.k=k\n",
        "    self.distance_func=distance_func\n",
        "    self.beta=beta\n",
        "\n",
        "  def fit(self,X,iterations=5):\n",
        "    indices = np.arange(X.shape[0])\n",
        "    sample_indices = np.random.choice(indices,size=self.k,replace=False)\n",
        "    self.means = X[sample_indices]\n",
        "\n",
        "    for i in range(iterations):\n",
        "      y_hat = self.Predict(X)\n",
        "      self.means=[]\n",
        "      for j in range(self.k):\n",
        "        mean=np.mean(X[y_hat==j], axis=0)\n",
        "        self.means.append(mean)\n",
        "      self.means=np.vstack(self.means)\n",
        "\n",
        "    y_hat=self.Predict(X)\n",
        "    plt.figure(figsize=(10,7))\n",
        "    plt.scatter(X[:,0],X[:,1],s=1,c=y_hat)\n",
        "    plt.scatter(self.means[:,0],self.means[:,1], c='k',s=10)\n",
        "\n",
        "    return y_hat\n",
        "  \n",
        "  def Predict(self,X):\n",
        "    dist=self.distance_func(X,self.means,self.beta)\n",
        "    y_hat = np.argmin(dist,axis=1)\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "_1atETCvtZsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_tweets=[]\n",
        "for i in range(len(X)):\n",
        "  vector_tweets.append(np.hstack(My_NLP_dict[i]['vector'].numpy())) \n",
        "vector_tweets = np.vstack(vector_tweets)"
      ],
      "metadata": {
        "id": "hkFl6v-xtbsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_tweets[1]"
      ],
      "metadata": {
        "id": "X1FfZy97tc72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_tweets[1:3]"
      ],
      "metadata": {
        "id": "UE1iaA0Qte3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweetcluster = K_Means(k=12)"
      ],
      "metadata": {
        "id": "ToVfLRKBtggG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweetcluster.fit(vector_tweets, iterations = 20)"
      ],
      "metadata": {
        "id": "RaFckSg8tg5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = tweetcluster.Predict(vector_tweets)"
      ],
      "metadata": {
        "id": "uzaHGcEbtiW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat"
      ],
      "metadata": {
        "id": "EJ7SMVp6tk7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (len(X)):\n",
        "  print(y_hat[i], \" \", My_NLP_dict[i][\"sentence\"])"
      ],
      "metadata": {
        "id": "26Y2PjFEtmXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_cluster_soft = K_Means(k=12, distance_func = Responsability, beta = 1.6)"
      ],
      "metadata": {
        "id": "s_TtgARXtoJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat2 = tweet_cluster_soft.fit(vector_tweets, iterations = 10)"
      ],
      "metadata": {
        "id": "ng_yOfZZtpXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "for i in range(len(X)):\n",
        "  sentences.append(My_NLP_dict[i]['sentence'])"
      ],
      "metadata": {
        "id": "Q1aUNPvbtqwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(y_hat, sentences)"
      ],
      "metadata": {
        "id": "FytUaGZ8tsFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "njPsx9fXttRn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}